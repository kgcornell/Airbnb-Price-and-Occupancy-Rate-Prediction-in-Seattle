{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import math\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3818, 16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = pd.read_csv('listings.csv')\n",
    "list2 = pd.read_csv('listings2.csv')\n",
    "nbhd = pd.read_csv('neighbourhoods.csv')\n",
    "calendar = pd.read_csv('calendar.csv')\n",
    "review1 = pd.read_csv('reviews.csv')\n",
    "review2 = pd.read_csv('reviews2.csv')\n",
    "amenities = pd.read_csv('Cata Feature CSV/amenities.csv')\n",
    "Integrate = pd.read_csv('Cata Feature CSV/IntegrateCata.csv')\n",
    "amenities.shape\n",
    "property_type = pd.read_csv('property_type.csv')\n",
    "property_type.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  2. Data Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(list1.columns.values)\n",
    "# print(list1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(list2.columns.values)\n",
    "# print(list2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join list1 and list2 to create the x vector\n",
    "X = pd.concat([list1, list2], axis=1)\n",
    "# print(X.shape)\n",
    "# X.info(verbose = True, null_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X=X[X['property_type'].notnull()]\n",
    "# X['property_type'].unique()\n",
    "# # Increasing the feature space\n",
    "# i=0\n",
    "# for item in X['property_type'].unique():\n",
    "#     a=X['property_type'] == X['property_type'].unique()[i]\n",
    "#     X[item] = a.map(lambda x: 1 if x == True else 0)\n",
    "#     i=i+1\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3818, 40)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1=X.loc[:, X.dtypes == np.float64] #Extracting columns with values of type float \n",
    "Z2=X.loc[:, X.dtypes == np.int64]   #Extracting columns with values of type int\n",
    "Z3=X.loc[:, X.dtypes == np.object]   #Extracting columns with values of type categorical\n",
    "X_numeric=pd.concat([Z1,Z2], axis=1)\n",
    "X_cat = Z3\n",
    "X_numeric.shape\n",
    "# X_numeric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"X_numeric has duplicate columns. The code below removes the duplicate columns\"\"\"\n",
    "_, i = np.unique(X_numeric.columns, return_index=True)\n",
    "X_Num_Cov=X_numeric.iloc[:, i]\n",
    "X_Num_Cov.to_csv('Numerical_FS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, i = np.unique(X_cat.columns, return_index=True)\n",
    "X_cat=X_cat.iloc[:, i]\n",
    "# print(X_cat.describe())\n",
    "# print(X_cat.info())\n",
    "X_cat.to_csv('Catgorical_FS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating X and Y for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_select = X_Num_Cov[['reviews_per_month','accommodates','bathrooms','bedrooms','beds','guests_included','latitude','longitude','price']]\n",
    "X_select['avail'] = list2[['availability_365']]\n",
    "X_select = pd.concat([X_select, amenities, Integrate, property_type], axis=1)\n",
    "X_select = X_select.dropna()\n",
    "Y = X_select['reviews_per_month']\n",
    "X_select = X_select.drop(['reviews_per_month','Unnamed: 0'], axis = 1)\n",
    "# print(X_select.info())\n",
    "# X_select.columns\n",
    "# X_select.info(verbose = True, null_counts = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      price\n",
      "1274    165\n",
      "765     120\n",
      "625      45\n",
      "1828     28\n",
      "448      95\n",
      "85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.0700000000000003"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_test = 0.1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_select, Y, test_size=num_test, random_state=23)\n",
    "X_train.shape\n",
    "y_train = y_train.to_frame()\n",
    "y_test = y_test.to_frame()\n",
    "print(X_train[['price']].head())\n",
    "print(X_train.price[0])\n",
    "y_train.reviews_per_month[0]\n",
    "\n",
    "# X_test.shape\n",
    "# property_type.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression with Validation Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.283196304922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28319630492218317"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Ridge(alpha=1.0)\n",
    "clf.fit(X_train, y_train) \n",
    "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
    "      normalize=True, random_state=None, solver='auto', tol=0.001)\n",
    "ridgeTrain = clf.predict(X_train)\n",
    "ridgeTest = clf.predict(X_test)\n",
    "print(clf.score(X_test, y_test))\n",
    "r2_score( y_test,ridgeTest)\n",
    "\n",
    "\n",
    "\n",
    "# param_range = np.logspace(-7, 3, 3)\n",
    "# # train_scores, test_scores = validation_curve(\n",
    "# #     Ridge(), X_select, Y, param_name=\"alpha\", param_range=param_range,\n",
    "# #     cv=10, scoring=\"accuracy\", n_jobs=1)\n",
    "# train_scores, test_scores = validation_curve(Ridge(), X_select, Y, \"alpha\",param_range,cv=10, n_jobs=1)\n",
    "# train_scores_mean = np.mean(train_scores, axis=1)\n",
    "# train_scores_std = np.std(train_scores, axis=1)\n",
    "# test_scores_mean = np.mean(test_scores, axis=1)\n",
    "# test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# plt.title(\"Validation Curve with Ridge Regression\")\n",
    "# plt.xlabel(\"alpha\")\n",
    "# plt.ylabel(\"Score\")\n",
    "# plt.ylim(0.0, 0.5)\n",
    "# lw = 2\n",
    "# plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "#              color=\"darkorange\", lw=lw)\n",
    "# plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "#                  train_scores_mean + train_scores_std, alpha=0.2,\n",
    "#                  color=\"darkorange\", lw=lw)\n",
    "# plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "#              color=\"navy\", lw=lw)\n",
    "# plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "#                  test_scores_mean + test_scores_std, alpha=0.2,\n",
    "#                  color=\"navy\", lw=lw)\n",
    "# plt.legend(loc=\"best\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regr = linear_model.LinearRegression()\n",
    "# regr.fit(X_train, y_train) \n",
    "# print(regr.score(X_test, y_test))\n",
    "# regr.get_params()\n",
    "# regr.coef_\n",
    "\n",
    "# title = \"Learning Curves (Linear Regression)\"\n",
    "# # SVC is more expensive so we do a lower number of CV iterations:\n",
    "# cv = ShuffleSplit(n_splits=20, test_size=0.2, random_state=0)\n",
    "# estimator = regr\n",
    "# plot_learning_curve(estimator, title, X_select, Y, (0, 1.01), cv=cv, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clf = linear_model.Lasso(alpha=0.0, fit_intercept=True, \n",
    "#                          normalize=True, precompute=False, copy_X=True, \n",
    "#                          positive=False, random_state=None,tol=0.001)\n",
    "# clf.fit(X_train, y_train) \n",
    "# print(clf.score(X_test, y_test))\n",
    "\n",
    "# param_range = np.logspace(-7, 3, 3)\n",
    "# # train_scores, test_scores = validation_curve(\n",
    "# #     Ridge(), X_select, Y, param_name=\"alpha\", param_range=param_range,\n",
    "# #     cv=10, scoring=\"accuracy\", n_jobs=1)\n",
    "# train_scores, test_scores = validation_curve(Lasso(), X_select, Y, \"alpha\",param_range,cv=10, n_jobs=1)\n",
    "# train_scores_mean = np.mean(train_scores, axis=1)\n",
    "# train_scores_std = np.std(train_scores, axis=1)\n",
    "# test_scores_mean = np.mean(test_scores, axis=1)\n",
    "# test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# plt.title(\"Validation Curve with Lasso Regression\")\n",
    "# plt.xlabel(\"alpha\")\n",
    "# plt.ylabel(\"Score\")\n",
    "# plt.ylim(0.0, 0.5)\n",
    "# lw = 2\n",
    "# plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "#              color=\"darkorange\", lw=lw)\n",
    "# plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "#                  train_scores_mean + train_scores_std, alpha=0.2,\n",
    "#                  color=\"darkorange\", lw=lw)\n",
    "# plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "#              color=\"navy\", lw=lw)\n",
    "# plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "#                  test_scores_mean + test_scores_std, alpha=0.2,\n",
    "#                  color=\"navy\", lw=lw)\n",
    "# plt.legend(loc=\"best\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:10: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.31818768454812529"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "# RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
    "#            max_features='auto', max_leaf_nodes=None,\n",
    "#            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#            min_samples_leaf=1, min_samples_split=2,\n",
    "#            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "#            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "clf = RandomForestRegressor(max_depth=20, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "# print(clf.feature_importances_)\n",
    "RFTrain = clf.predict(X_train)\n",
    "RFTest = clf.predict(X_test)\n",
    "clf.score(X_test, y_test)\n",
    "r2_score(RFTest, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.14028045358039121"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "clf = MLPRegressor(solver='adam', alpha=1e-4,hidden_layer_sizes=(1000, 6), random_state=1)\n",
    "clf.fit(X_train, y_train)                         \n",
    "# MLPRegressor(hidden_layer_sizes=(100, ), activation=’relu’, solver=’adam’, alpha=0.0001, batch_size=’auto’, learning_rate=’constant’, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "ANNTrain = clf.predict(X_train)\n",
    "ANNTest = clf.predict(X_test)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.31704104434860447"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "X_train = preprocessing.scale(X_train)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "# You can experiment with many other options here, using the same .fit() and .predict()\n",
    "# methods; see http://scikit-learn.org\n",
    "# This example uses the current build of XGBoost, from https://github.com/dmlc/xgboost\n",
    "gbm = xgb.XGBRegressor(max_depth=3, n_estimators=400, learning_rate=0.05).fit(X_train, y_train)\n",
    "X_test = preprocessing.scale(X_test)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "predictions = gbm.predict(X_test)\n",
    "XGTrain = gbm.predict(X_train)\n",
    "XGTest = gbm.predict(X_test)\n",
    "gbm.score(X_test, y_test)\n",
    "r2_score(XGTest, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "# create model\n",
    "tmodel = Sequential()\n",
    "tmodel.add(Dense(input_dim=X_train.shape[1], units=128,\n",
    "                 kernel_initializer='normal', bias_initializer='zeros'))\n",
    "tmodel.add(Activation('relu'))\n",
    "\n",
    "for i in range(0, 8):\n",
    "    tmodel.add(Dense(units=100, kernel_initializer='normal',\n",
    "                     bias_initializer='zeros'))\n",
    "    tmodel.add(Activation('relu'))\n",
    "    tmodel.add(Dropout(.25))\n",
    "\n",
    "tmodel.add(Dense(units=1))\n",
    "tmodel.add(Activation('linear'))\n",
    "\n",
    "tmodel.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "\n",
    "tmodel.fit(X_train.values, y_train.values, epochs=100, verbose=0)\n",
    "\n",
    "DNTrain = tmodel.predict(X_train.values)\n",
    "DNTest = tmodel.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = tmodel.predict(X_test.values)\n",
    "loss_and_metrics = tmodel.evaluate(X_test.values, y_test.values, batch_size=128)\n",
    "print(loss_and_metrics)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(y_test.values, classes))\n",
    "r2_score(y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "rng = np.random.RandomState(1)\n",
    "regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=8),\n",
    "                          n_estimators=100, random_state=rng)\n",
    "\n",
    "regr_2.fit(X_train, y_train)\n",
    "regr_2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ensambling／Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold;\n",
    "ntrain = X_train.shape[0]\n",
    "ntest = X_test.shape[0]\n",
    "SEED = 0 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n",
    "# X_train.reset_index(inplace=True)\n",
    "# y_train.reset_index(inplace=True)\n",
    "# X_test.reset_index(inplace=True)\n",
    "# y_test.reset_index(inplace=True)\n",
    "X_train =X_train.as_matrix()\n",
    "y_train =y_train.as_matrix()\n",
    "X_test =X_test.as_matrix()\n",
    "y_test =y_test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.fit(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te).flatten()\n",
    "        oof_test_skf[i, :] = clf.predict(x_test).flatten()\n",
    "    xx = oof_test_skf.mean(axis=0)\n",
    "    oof_test[:] = xx.flatten()\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rr = Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
    "      normalize=True, random_state=None, solver='auto', tol=0.001)\n",
    "rr_oof_train, rr_oof_test = get_oof(rr, X_train, y_train, X_test) # Ridge Regression\n",
    "ann = MLPRegressor(solver='adam', alpha=1e-4,hidden_layer_sizes=(1000, 6), random_state=1)\n",
    "rf_oof_train, rf_oof_test = get_oof(ann,X_train, y_train, X_test) # ANN\n",
    "ada = AdaBoostRegressor(DecisionTreeRegressor(max_depth=8),\n",
    "                          n_estimators=100, random_state=rng)\n",
    "ada_oof_train, ada_oof_test = get_oof(ada, X_train, y_train, X_test) # AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.concatenate(( rr_oof_train, rf_oof_train, ada_oof_train), axis=1)\n",
    "x_test = np.concatenate(( rr_oof_test, rf_oof_test, ada_oof_test), axis=1)\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "# x_train = preprocessing.scale(x_train)\n",
    "x_test = pd.DataFrame(x_test)\n",
    "x_train = pd.DataFrame(x_train)\n",
    "# You can experiment with many other options here, using the same .fit() and .predict()\n",
    "# methods; see http://scikit-learn.org\n",
    "# This example uses the current build of XGBoost, from https://github.com/dmlc/xgboost\n",
    "gbm = xgb.XGBRegressor(max_depth=3, n_estimators=400, learning_rate=0.05).fit(x_train, y_train)\n",
    "# X_test = preprocessing.scale(X_test)\n",
    "# X_test = pd.DataFrame(X_test)\n",
    "\n",
    "predictions = gbm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFTrain = pd.DataFrame(RFTrain)\n",
    "RFTest = pd.DataFrame(RFTest)\n",
    "XGTrain = pd.DataFrame(XGTrain)\n",
    "XGTest = pd.DataFrame(XGTest)\n",
    "ANNTrain = pd.DataFrame(ridgeTrain)\n",
    "ANNTest = pd.DataFrame(ridgeTest)\n",
    "x_train = np.concatenate(( ridgeTrain, ANNTrain, RFTrain ,XGTrain), axis=1)\n",
    "x_test = np.concatenate(( ridgeTest, ANNTest, RFTest, XGTest), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30312291492016064"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "rng = np.random.RandomState(1)\n",
    "regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=8),\n",
    "                          n_estimators=100, random_state=rng)\n",
    "\n",
    "regr_2.fit(x_train, y_train)\n",
    "pred = regr_2.predict(x_test)\n",
    "pred = pd.DataFrame(pred)\n",
    "regr_2.score(x_test, y_test)\n",
    "r2_score( y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(318, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
